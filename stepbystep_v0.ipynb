{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmDeXrhAJack4BbsZJ9dWx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andreaslanga/Lecture-notes/blob/main/stepbystep_v0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljU_JtuQTufh"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "class StepByStep(object):\n",
        "  def __init__(self, model, loss_fn, optimizer):\n",
        "    self.model = model\n",
        "    self.loss_fn = loss_fn\n",
        "    self.optimizer = optimizer\n",
        "    self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    self.model.to(self.device) # Send Model to right device right away\n",
        "\n",
        "    # Placeholder / Delayed Arguments\n",
        "    self.train_loader = None\n",
        "    self.val_loader = None\n",
        "    self.writer = None\n",
        "\n",
        "    # Variables\n",
        "    self.losses = []\n",
        "    self.val_losses = []\n",
        "    self.total_epochs = 0\n",
        "\n",
        "    # Functions\n",
        "    self.train_step_fn = self._make_train_step_fn()\n",
        "    self.val_step_fn = self._make_val_step_fn()\n",
        "\n",
        "  def to(self, device):\n",
        "    try:\n",
        "      self.device = device\n",
        "      self.model.to(self.device)\n",
        "    except RuntimeError:\n",
        "      self.device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "      print(f\"Couldn't send it to {device}, sending it to {self.device} instead.\")\n",
        "      self.model.to(self.device)\n",
        "\n",
        "  def set_loaders(self, train_loader, val_loader):\n",
        "    self.train_loader = train_loader\n",
        "    self.val_loader = val_loader\n",
        "\n",
        "  def set_tensorboard(self, name, folder = 'runs'):\n",
        "    suffix = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    self.writer = SummaryWriter(f'{folder}/{name}_{suffix}')\n",
        "\n",
        "  def _make_train_step_fn(self):\n",
        "    # Builds function that performs a step in the training loop\n",
        "    def perform_train_step_fn(x,y):\n",
        "      self.model.train()\n",
        "      yhat = self.model(x)\n",
        "      loss = self.loss_fn(yhat, y)\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "      self.optimizer.zero_grad()\n",
        "      return loss.item()\n",
        "\n",
        "    return perform_train_step_fn\n",
        "\n",
        "  def _make_val_step_fn(self):\n",
        "    def perform_val_step_fn(x,y):\n",
        "      self.model.eval()\n",
        "      yhat = self.model(x)\n",
        "      loss = self.loss_fn(yhat, y)\n",
        "      return loss.item()\n",
        "\n",
        "    return perform_val_step_fn\n",
        "\n",
        "  def _mini_batch(self, validation = False):\n",
        "    if validation:\n",
        "      data_loader = self.val_loader\n",
        "      step_fn = self.val_step_fn\n",
        "    else:\n",
        "      data_loader = self.train_loader\n",
        "      step_fn = self.train_step_fn\n",
        "\n",
        "    if data_loader is None:\n",
        "      return None\n",
        "\n",
        "    mini_batch_losses = []\n",
        "    for x_batch, y_batch in data_loader:\n",
        "      x_batch = x_batch.to(self.device)\n",
        "      y_batch = y_batch.to(self.device)\n",
        "      mini_batch_loss = step_fn(x_batch, y_batch)\n",
        "      mini_batch_losses.append(mini_batch_loss)\n",
        "\n",
        "    loss = np.mean(mini_batch_losses)\n",
        "    return loss\n",
        "\n",
        "  def set_seed(self, seed=42):\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "  def train(self, n_epochs, seed=42):\n",
        "    #To ensure reproducability of the training process\n",
        "    self.set_seed(seed)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "      # Keep track of number of epochs\n",
        "      self.total_epochs += 1\n",
        "\n",
        "      # Performs training using mini-batches\n",
        "      loss = self._mini_batch(validation=False)\n",
        "      self.losses.append(loss)\n",
        "\n",
        "      # Validation\n",
        "      with torch.no_grad():\n",
        "        # Performs evaluation using mini-batches\n",
        "        val_loss = self._mini_batch(validation=True)\n",
        "        self.val_losses.append(val_loss)\n",
        "\n",
        "      if self.writer:\n",
        "        scalars = {'training':loss}\n",
        "        if val_loss is not None:\n",
        "          scalars.update({'validation':val_loss})\n",
        "        self.writer.add_scalars(main_tag='loss',\n",
        "                                tag_scalar_dict=scalars,\n",
        "                                global_step=epoch)\n",
        "        self.writer.close()\n",
        "\n",
        "  def save_checkpoint(self, filename):\n",
        "    checkpoint = {\n",
        "        'model_state_dict': self.model.state_dict(),\n",
        "        'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "        'epoch': self.total_epochs,\n",
        "        'loss': self.losses,\n",
        "        'val_loss': self.val_losses\n",
        "    }\n",
        "    torch.save(checkpoint, filename)\n",
        "\n",
        "  def load_checkpoint(self, filename):\n",
        "    checkpoint = torch.load(filename) # Loads dictionary\n",
        "\n",
        "    # Restore state for model & optimizer\n",
        "    self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    self.total_epochs = checkpoint['epoch']\n",
        "    self.losses = checkpoint['loss']\n",
        "    self.val_losses = checkpoint['val_loss']\n",
        "\n",
        "    self.model.train() # for resuming training\n",
        "\n",
        "  def predict(self, x):\n",
        "    self.model.eval()\n",
        "    # Takes a Numpy input and make it a float tensor\n",
        "    x_tensor = torch.as_tensor(x).float()\n",
        "    # Send Input to device + uses model for predictions\n",
        "    yhat_tensor = self.model(x_tensor.to(self.device))\n",
        "    # Set it back to train mode\n",
        "    self.model.train()\n",
        "    # Detaches it, brings it to CPU & back to numpy\n",
        "    return yhat_tensor.detach().cpu().numpy()\n",
        "\n",
        "  def plot_losses(self):\n",
        "    fig = plt.figure(figsize=(10,4))\n",
        "    plt.plot(self.losses, label='Training Loss', c='b')\n",
        "    plt.plot(self.val_losses, label='Validation Loss', c='r')\n",
        "    plt.yscale('log')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "  def add_graph(self):\n",
        "    # Fetches a single mini-batch so we can use add_graph\n",
        "    if self.train_loader and self.writer:\n",
        "      x_sample, y_sample = next(iter(self.train_loader))\n",
        "      self.writer.add_graph(self.model, x_sample.to(self.device))"
      ]
    }
  ]
}